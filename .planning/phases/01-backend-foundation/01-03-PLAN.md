---
phase: 01-backend-foundation
plan: 03
type: execute
wave: 2
depends_on:
  - "01-01"
files_modified:
  - apps/api/app/scraper/scraper.py
  - apps/api/app/graph/repository.py
  - apps/api/tests/test_scraper.py
autonomous: true
requirements:
  - AI-02
  - AI-03
  - SEC-02

must_haves:
  truths:
    - "scrape_url() extracts readable text from HTML, strips scripts/nav/footer noise, and returns up to 32,000 characters"
    - "scrape_url() raises HTTPException(400) with error='scrape_failed' when extracted text is under 500 characters"
    - "scrape_url() uses allow_redirects=False to prevent redirect-based SSRF bypass"
    - "persist_graph() uses only parameterized Cypher — no string formatting anywhere in repository.py"
    - "All Neo4j queries in repository.py use $param syntax — grep for .format( and f-strings returns no results"
  artifacts:
    - path: "apps/api/app/scraper/scraper.py"
      provides: "SSRF-aware URL scraper with BeautifulSoup HTML extraction"
      contains: "allow_redirects=False"
    - path: "apps/api/app/graph/repository.py"
      provides: "Neo4j CRUD operations with parameterized Cypher"
      contains: "UNWIND $nodes"
    - path: "apps/api/tests/test_scraper.py"
      provides: "Pytest tests for BeautifulSoup extraction and redirect handling"
      contains: "test_extracts_article_text"
  key_links:
    - from: "apps/api/app/scraper/scraper.py"
      to: "apps/api/app/scraper/ssrf.py"
      via: "validate_url() called before requests.get()"
      pattern: "validate_url"
    - from: "apps/api/app/graph/repository.py"
      to: "neo4j driver"
      via: "driver injected from app.state (Plan 01 singleton)"
      pattern: "def persist_graph\\(driver"
---

<objective>
Implement the BeautifulSoup URL scraper (`scraper.py`) and the parameterized Neo4j repository (`repository.py`). These two pieces are independent — scraper handles inbound content extraction, repository handles outbound graph persistence. Both depend only on the Plan 01 skeleton.

Purpose: The scraper feeds content to GPT-4o (Plan 05). The repository persists the resulting graph. Both are required before the generate endpoint (Plan 05) can wire them together.

Output: `apps/api/app/scraper/scraper.py` with `scrape_url()`, `apps/api/app/graph/repository.py` with `persist_graph()`, and test coverage for the scraper in `tests/test_scraper.py`.
</objective>

<execution_context>
@/Users/uzi/.claude/get-shit-done/workflows/execute-plan.md
@/Users/uzi/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/phases/01-backend-foundation/01-CONTEXT.md
@.planning/phases/01-backend-foundation/01-RESEARCH.md
@.planning/phases/01-backend-foundation/01-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: BeautifulSoup URL scraper with SSRF guard</name>
  <files>
    apps/api/app/scraper/scraper.py
    apps/api/tests/test_scraper.py
  </files>
  <action>
Implement `scrape_url()` in `apps/api/app/scraper/scraper.py`. This function accepts a validated HTTPS URL, fetches it with `allow_redirects=False` (SSRF redirect bypass prevention), extracts text via BeautifulSoup, and returns up to 32,000 characters.

**Implementation of `apps/api/app/scraper/scraper.py`:**

```python
import requests
from bs4 import BeautifulSoup
from fastapi import HTTPException

from app.scraper.ssrf import validate_url

# Spoof a realistic Chrome User-Agent to pass basic bot detection on news sites
# (TechCrunch, CoinDesk, The Block) — per CONTEXT.md Scraper Robustness decision
CHROME_UA = (
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
    "AppleWebKit/537.36 (KHTML, like Gecko) "
    "Chrome/120.0.0.0 Safari/537.36"
)

MAX_CONTENT_CHARS = 32_000  # AI-02: cap before sending to GPT-4o
MIN_CONTENT_CHARS = 500     # CONTEXT.md: <500 chars → scrape_failed


def scrape_url(url: str) -> str:
    """
    Fetches a public HTTPS URL, strips boilerplate HTML, and returns
    extracted text (up to 32,000 chars) for GPT-4o processing (AI-02).

    SSRF protection (SEC-01):
    - validate_url() is called first — raises 400 if URL is private/blocked
    - allow_redirects=False prevents redirect chains to private IPs
    - Redirect Location headers are validated through validate_url() before following

    Source type handling (AI-03):
    - This function is ONLY called when input is a URL (source_type="url")
    - Raw text input bypasses this function entirely (handled in generate/service.py)

    Raises:
        HTTPException(400): URL is private/blocked (from validate_url)
        HTTPException(400): Fetched content is too short (paywalled/empty page)
        HTTPException(400): HTTP error (4xx, 5xx from target site)
        HTTPException(503): Network timeout or connection error
    """
    # SSRF guard — resolves hostname and validates IP before any network request
    validate_url(url)

    try:
        response = requests.get(
            url,
            headers={"User-Agent": CHROME_UA},
            timeout=10,          # 10s timeout — fail fast per CONTEXT.md
            allow_redirects=False,  # CRITICAL: validate redirects manually (SEC-01)
        )

        # Handle redirects manually to validate each redirect target
        redirect_count = 0
        while response.is_redirect and redirect_count < 5:
            redirect_url = response.headers.get("Location", "")
            if not redirect_url.startswith("http"):
                # Relative redirect — reconstruct absolute URL
                from urllib.parse import urljoin
                redirect_url = urljoin(url, redirect_url)
            # Validate the redirect target through SSRF guard
            validate_url(redirect_url)
            response = requests.get(
                redirect_url,
                headers={"User-Agent": CHROME_UA},
                timeout=10,
                allow_redirects=False,
            )
            redirect_count += 1

        response.raise_for_status()

    except requests.Timeout:
        raise HTTPException(status_code=503, detail={
            "error": "service_unavailable",
            "message": "URL fetch timed out — try again or paste the text directly",
        })
    except requests.ConnectionError:
        raise HTTPException(status_code=503, detail={
            "error": "service_unavailable",
            "message": "Could not connect to URL — try pasting the text instead",
        })
    except requests.HTTPError as e:
        raise HTTPException(status_code=400, detail={
            "error": "scrape_failed",
            "message": "Couldn't read that URL — try pasting the text instead",
        })

    text = _extract_text(response.text)

    # Low content yield detection — paywalled or near-empty pages
    if len(text) < MIN_CONTENT_CHARS:
        raise HTTPException(status_code=400, detail={
            "error": "scrape_failed",
            "message": "Couldn't read that URL — try pasting the text instead",
        })

    return text[:MAX_CONTENT_CHARS]


def _extract_text(html: str) -> str:
    """
    Extracts readable text from HTML using BeautifulSoup with lxml parser.
    Strips: script, style, nav, footer, header tags.
    Extracts: h1/h2/h3 headings, <article> body, <p> paragraphs.
    Uses lxml for performance on large news articles (vs html.parser).
    """
    soup = BeautifulSoup(html, "lxml")

    # Remove boilerplate noise
    for tag in soup(["script", "style", "nav", "footer", "header", "aside"]):
        tag.decompose()

    parts = []
    # Headings provide structural context for GPT-4o
    for heading in soup.find_all(["h1", "h2", "h3"]):
        text = heading.get_text(strip=True)
        if text:
            parts.append(text)
    # Article body (many news sites wrap content in <article>)
    for article in soup.find_all("article"):
        text = article.get_text(separator=" ", strip=True)
        if text:
            parts.append(text)
    # Paragraph fallback (catches sites without <article>)
    for p in soup.find_all("p"):
        text = p.get_text(strip=True)
        if text:
            parts.append(text)

    return " ".join(parts)
```

**Write `apps/api/tests/test_scraper.py`:**

```python
import pytest
from unittest.mock import patch, MagicMock
from fastapi import HTTPException
from app.scraper.scraper import scrape_url, _extract_text


class TestExtractText:
    def test_extracts_article_text(self):
        html = "<html><body><article><p>Paradigm led the $50M round.</p></article></body></html>"
        result = _extract_text(html)
        assert "Paradigm" in result
        assert "$50M" in result

    def test_strips_script_tags(self):
        html = "<html><body><script>evil();</script><p>Real content here.</p></body></html>"
        result = _extract_text(html)
        assert "evil" not in result
        assert "Real content here" in result

    def test_strips_nav_footer(self):
        html = "<html><body><nav>Menu items</nav><p>Article text</p><footer>Copyright</footer></body></html>"
        result = _extract_text(html)
        assert "Menu items" not in result
        assert "Copyright" not in result
        assert "Article text" in result

    def test_extracts_headings(self):
        html = "<html><body><h1>Funding Round Announced</h1><p>Details here.</p></body></html>"
        result = _extract_text(html)
        assert "Funding Round Announced" in result

    def test_caps_at_32000_chars(self):
        long_html = "<html><body>" + "<p>" + "a" * 100 + "</p>" * 400 + "</body></html>"
        # scrape_url caps, but _extract_text itself does not; test via scrape_url
        result = _extract_text(long_html)
        # The raw extraction may exceed 32k; capping is done in scrape_url
        assert isinstance(result, str)


class TestScrapeUrl:
    @patch("app.scraper.scraper.validate_url")
    @patch("app.scraper.scraper.requests.get")
    def test_returns_extracted_text(self, mock_get, mock_validate):
        mock_response = MagicMock()
        mock_response.is_redirect = False
        mock_response.text = (
            "<html><body><article><p>" + "Paradigm Capital led a $50M Series B. " * 20 + "</p></article></body></html>"
        )
        mock_response.raise_for_status.return_value = None
        mock_get.return_value = mock_response

        result = scrape_url("https://techcrunch.com/article")
        assert "Paradigm" in result
        mock_validate.assert_called_once_with("https://techcrunch.com/article")

    @patch("app.scraper.scraper.validate_url")
    @patch("app.scraper.scraper.requests.get")
    def test_rejects_low_content_page(self, mock_get, mock_validate):
        mock_response = MagicMock()
        mock_response.is_redirect = False
        mock_response.text = "<html><body><p>Short.</p></body></html>"
        mock_response.raise_for_status.return_value = None
        mock_get.return_value = mock_response

        with pytest.raises(HTTPException) as exc_info:
            scrape_url("https://paywalled.com/article")
        assert exc_info.value.status_code == 400
        assert exc_info.value.detail["error"] == "scrape_failed"

    @patch("app.scraper.scraper.validate_url")
    @patch("app.scraper.scraper.requests.get")
    def test_uses_allow_redirects_false(self, mock_get, mock_validate):
        mock_response = MagicMock()
        mock_response.is_redirect = False
        mock_response.text = "<html><body>" + "<p>Content. " * 100 + "</p></body></html>"
        mock_response.raise_for_status.return_value = None
        mock_get.return_value = mock_response

        scrape_url("https://example.com/article")

        # Verify allow_redirects=False was passed to requests.get
        call_kwargs = mock_get.call_args[1]
        assert call_kwargs.get("allow_redirects") is False

    @patch("app.scraper.scraper.validate_url")
    @patch("app.scraper.scraper.requests.get")
    def test_timeout_returns_503(self, mock_get, mock_validate):
        import requests as req_lib
        mock_get.side_effect = req_lib.Timeout()

        with pytest.raises(HTTPException) as exc_info:
            scrape_url("https://slow-site.com/article")
        assert exc_info.value.status_code == 503
```
  </action>
  <verify>
    <automated>cd "/Users/uzi/Documents/Git Projects/instagraph-vc/apps/api" && uv run pytest tests/test_scraper.py -v</automated>
    <manual>Confirm `allow_redirects=False` is present in `scraper.py`. Confirm `validate_url()` is called before `requests.get()` — SSRF guard is the first line of `scrape_url()`. Confirm no `.format(` or f-strings in `scraper.py`.</manual>
  </verify>
  <done>
    - `pytest tests/test_scraper.py` passes with 0 failures
    - `scrape_url()` calls `validate_url(url)` as its first action (SSRF guard in place)
    - `requests.get()` called with `allow_redirects=False`
    - Redirect chain validated through `validate_url()` before following
    - Content < 500 chars raises HTTPException(400) with `error: "scrape_failed"`
    - Returned text is capped at 32,000 characters
  </done>
</task>

<task type="auto">
  <name>Task 2: Parameterized Neo4j graph repository</name>
  <files>
    apps/api/app/graph/repository.py
  </files>
  <action>
Implement `apps/api/app/graph/repository.py` with `persist_graph()`. This function receives the GPT-4o output and writes nodes and relationships to Neo4j using exclusively parameterized Cypher.

CRITICAL SECURITY REQUIREMENT (SEC-02): Zero string formatting in any Cypher query. No `.format()`, no f-strings, no `%` substitution. Every dynamic value must go through `$param` parameter syntax. The existing codebase's `drivers/neo4j.py` at line 71-77 used `.format()` for SKIP/LIMIT — that is the Cypher injection vulnerability this plan eliminates.

**Implementation of `apps/api/app/graph/repository.py`:**

```python
from typing import Any
from neo4j import Driver


def persist_graph(
    driver: Driver,
    session_id: str,
    nodes: list[dict[str, Any]],
    edges: list[dict[str, Any]],
) -> None:
    """
    Persists graph nodes and relationships to Neo4j (SEC-02, AI-01).

    Security: ALL Cypher queries use $param syntax — zero string interpolation.
    Scope: All nodes/edges get session_id property for query isolation (CONTEXT.md).
    Pattern: Uses UNWIND for batch writes — single query per node/edge batch.

    Each node dict: {id, label, type, properties}
    Each edge dict: {source, target, relationship}
    """
    with driver.session() as session:
        # Persist nodes — parameterized UNWIND batch insert
        # NOTE: Entity label is constant (not user-supplied) so dynamic label is safe here.
        # The type property (Investor/Project/Round/Narrative/Person) is stored as a
        # property, not a label — avoids dynamic label injection entirely.
        session.run(
            """
            UNWIND $nodes AS node
            CREATE (n:Entity {
                id:         node.id,
                label:      node.label,
                type:       node.type,
                properties: node.properties,
                session_id: $session_id
            })
            """,
            nodes=nodes,
            session_id=session_id,
        )

        # Persist relationships — parameterized UNWIND batch insert
        # Relationship type is stored as a property, not a dynamic Cypher label,
        # to maintain strict parameterization (no apoc.create.relationship needed).
        session.run(
            """
            UNWIND $edges AS edge
            MATCH (s:Entity {id: edge.source, session_id: $session_id})
            MATCH (t:Entity {id: edge.target, session_id: $session_id})
            CREATE (s)-[r:RELATES_TO {
                type:       edge.relationship,
                session_id: $session_id
            }]->(t)
            """,
            edges=edges,
            session_id=session_id,
        )


def get_graph_by_session(driver: Driver, session_id: str) -> dict[str, list]:
    """
    Retrieves all nodes and relationships for a session_id.
    Used for the API response and for Phase 3 history retrieval.
    Fully parameterized — session_id passed as $session_id parameter.
    """
    with driver.session() as session:
        # Fetch nodes
        node_result = session.run(
            """
            MATCH (n:Entity {session_id: $session_id})
            RETURN n.id AS id, n.label AS label, n.type AS type,
                   n.properties AS properties, n.session_id AS session_id
            """,
            session_id=session_id,
        )
        nodes = [dict(record) for record in node_result]

        # Fetch edges
        edge_result = session.run(
            """
            MATCH (s:Entity {session_id: $session_id})-[r:RELATES_TO]->(t:Entity)
            RETURN s.id AS source, t.id AS target, r.type AS relationship
            """,
            session_id=session_id,
        )
        edges = [dict(record) for record in edge_result]

    return {"nodes": nodes, "edges": edges}
```

**Verification (automated scan):**
After writing, run a grep to confirm there is zero string formatting in repository.py:
```bash
grep -n "\.format\(" apps/api/app/graph/repository.py
grep -n 'f"' apps/api/app/graph/repository.py
grep -n "f'" apps/api/app/graph/repository.py
```
All three must return empty (no matches).

**Note:** The relationship type (LED, INVESTED_IN, etc.) is stored as a property (`r.type`) rather than a dynamic Cypher relationship type. This avoids the need for `apoc.create.relationship()` and maintains strict parameterization. The Cytoscape UI in Phase 2 reads the `type` property for styling, not the Cypher label.
  </action>
  <verify>
    <automated>cd "/Users/uzi/Documents/Git Projects/instagraph-vc/apps/api" && grep -rn "\.format(" app/graph/repository.py; grep -rn 'f"' app/graph/repository.py; grep -rn "f'" app/graph/repository.py; echo "Exit: $? (0 means no string formatting found)"</automated>
    <manual>Confirm `repository.py` has both `persist_graph()` and `get_graph_by_session()`. Confirm all Cypher queries use `$session_id`, `$nodes`, `$edges` parameter syntax. No `.format(`, f-strings, or `%` substitution anywhere.</manual>
  </verify>
  <done>
    - `apps/api/app/graph/repository.py` exists with `persist_graph()` and `get_graph_by_session()`
    - `grep -rn ".format(" apps/api/app/graph/repository.py` returns empty (0 matches)
    - `grep -rn 'f"' apps/api/app/graph/repository.py` returns empty (0 matches)
    - All Cypher uses `$param` syntax: `$nodes`, `$edges`, `$session_id`
    - `persist_graph()` accepts `driver` parameter (injected from Plan 01 singleton)
    - Both functions use `with driver.session() as session:` context manager
  </done>
</task>

</tasks>

<verification>
After both tasks complete:
1. `cd apps/api && uv run pytest tests/test_scraper.py -v` — all pass
2. `grep -rn ".format(" apps/api/app/graph/ apps/api/app/scraper/` — returns empty (SEC-02 clean)
3. `grep -rn 'f"' apps/api/app/graph/repository.py` — returns empty
4. `grep "allow_redirects=False" apps/api/app/scraper/scraper.py` — returns match
5. `grep "validate_url" apps/api/app/scraper/scraper.py` — returns match (SSRF guard in place)
</verification>

<success_criteria>
- `scrape_url()` correctly integrates with `validate_url()` from Plan 02 as SSRF guard
- `scrape_url()` uses `allow_redirects=False` and validates redirect Location headers
- `scrape_url()` returns up to 32,000 chars of extracted text for valid public URLs
- `persist_graph()` and `get_graph_by_session()` use only parameterized Cypher — no string interpolation anywhere in `repository.py`
- All tests in `test_scraper.py` pass
</success_criteria>

<output>
After completion, create `.planning/phases/01-backend-foundation/01-03-SUMMARY.md`
</output>
